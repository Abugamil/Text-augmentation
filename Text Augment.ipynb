{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import random \n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer #one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, SpatialDropout1D, Input, Bidirectional, Dropout\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typos(word,count):\n",
    "    repl_symb={'а':['в','п','к','м','с'],\n",
    "               'б':['ь','ю','л','д'],            \n",
    "               'в':['у','с','ч','ы','а'],\n",
    "               'г':['н','ш','о'],\n",
    "               'д':['щ','ж','л','б','ю'],\n",
    "               'е':['к','н','п'],\n",
    "               'ж':['д','э','з','ю'],\n",
    "               'з':['щ','ж','х'],\n",
    "               'и':['м','п','р','т'],\n",
    "               'й':['ц','ф'],\n",
    "               'к':['у','е','а'],\n",
    "               'л':['о','д','ш','ь','б'],\n",
    "               'м':['с','и','а','п'],\n",
    "               'н':['е','г','р'],\n",
    "               'о':['г','р','т','ь','л'],\n",
    "               'п':['а','м','и','р','е'],\n",
    "               'р':['н','п','и','т','о'],\n",
    "               'с':['ч','в','а','м'],\n",
    "               'т':['и','р','о','ь'],\n",
    "               'у':['ц','ы','в','в','к'],\n",
    "               'ф':['й','ы','я'],\n",
    "               'х':['з','э','ъ'],\n",
    "               'ц':['й','ф'],\n",
    "               'ч':['я','ы','в','с'],\n",
    "               'ш':['г','л','щ'],\n",
    "               'щ':['ш','д','з'],\n",
    "               'ъ':['х'],\n",
    "               'ы':['ц','ч','ф','в'],\n",
    "               'ь':['т','о','л','б'],\n",
    "               'э':['ж','х'],\n",
    "               'ю':['б','д','ж'],\n",
    "               'я':['ф','ы','ч']}\n",
    "    result=[]\n",
    "    l=len(word)\n",
    "    if count>=l:\n",
    "        for i in range(l):\n",
    "            for c in repl_symb[word[i]]:\n",
    "                result.append(word[:i]+c+word[i+1:])\n",
    "    else:\n",
    "        for cur in range(count):   \n",
    "            pos=np.random.randint(l)\n",
    "            for c in repl_symb[word[pos]]:                \n",
    "                result.append(word[:pos]+c+word[pos+1:])    \n",
    "       # result=random.choices(result, k = count)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_char(word,count):\n",
    "    result=[]\n",
    "    l=len(word)\n",
    "    if count>=l:\n",
    "        for i in range(l):\n",
    "            result.append(word[:i]+word[i+1:])\n",
    "    else:\n",
    "        for cur in range(count):            \n",
    "            pos=np.random.randint(l)\n",
    "            result.append(word[:pos]+word[pos+1:])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_char(word,count):\n",
    "    vocab='абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "    l_v=len(vocab)\n",
    "    l=len(word)\n",
    "    result=[]\n",
    "    if count>=l:\n",
    "        for i in range(l):\n",
    "            pos=np.random.randint(l_v)\n",
    "            result.append(word[:i]+vocab[pos]+word[i:])\n",
    "    else:\n",
    "        for cur in range(count):            \n",
    "            pos=np.random.randint(l)  \n",
    "            c_pos=np.random.randint(l_v)\n",
    "            result.append(word[:pos]+vocab[c_pos]+word[pos:])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_char(word,count):\n",
    "    result=[]\n",
    "    l=len(word)\n",
    "    if count>=l:\n",
    "        for i in range(1,l-1):\n",
    "            left=word[:i]\n",
    "            right=word[i:]\n",
    "            if len(left)>0 and len(right)>0:\n",
    "                result.append(left[:-1]+right[0]+left[-1]+right[1:])\n",
    "    else:\n",
    "        for cur in range(count): \n",
    "            pos=np.random.randint(l-2)+1\n",
    "            left=word[:pos]\n",
    "            right=word[pos:]\n",
    "            if len(left)>0 and len(right)>0:\n",
    "                result.append(left[:-1]+right[0]+left[-1]+right[1:])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mistake(word,config,count):\n",
    "    all_mistakes=[]\n",
    "    if config['typos']:\n",
    "        all_mistakes+=typos(word,count)\n",
    "    if config['delchar']:\n",
    "        all_mistakes+=del_char(word,count)    \n",
    "    if config['addchar']:\n",
    "        all_mistakes+=add_char(word,count)\n",
    "    if config['swapchar']:\n",
    "        all_mistakes+=swap_char(word,count)\n",
    "    return all_mistakes\n",
    "#random.choices(all_mistakes, k = count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['исседователей']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={'typos':True,'delchar':True,'addchar':True,'swapchar':True}\n",
    "make_mistake('исследователей',config,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub_replace(fstr, tstr, text):\n",
    "    reg = re.compile(r'('+fstr+')')\n",
    "    output = re.sub(reg, tstr, text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 'dog'\n",
    "rep = 'cat'\n",
    "text = 'dog fat dog cat dog cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat fat cat cat cat cat'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_replace(reg,rep,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sentence(sentence,config,new_examples,mistake_words):\n",
    "    reg=re.compile('^[а-я]+$')\n",
    "    words=re.findall(r\"[\\w']+\", sentence)    \n",
    "    words=[word for word in words if len(word)>3 and reg.match(word)]  \n",
    "    size=len(words)\n",
    "    ans=[]\n",
    "    for i in range(new_examples):\n",
    "        bag=[]\n",
    "        miswords=[]\n",
    "        n=size//3\n",
    "        num_mistakes=min(mistake_words,n)        \n",
    "        newsentence=sentence\n",
    "        for mistakes in range(num_mistakes):\n",
    "            pos=0\n",
    "            if size>0:\n",
    "                pos=np.random.randint(size)                   \n",
    "            else:\n",
    "                break\n",
    "            word=words.pop(pos)\n",
    "            size=len(words)            \n",
    "            if word not in bag and word in aug_words:\n",
    "                bag.append(word)\n",
    "                miswords=miswords+random.choices(aug_words[word], k = 1)\n",
    "                #print(miswords)\n",
    "                #make_mistake(word,config,1)                    \n",
    "            else: \n",
    "                continue \n",
    "        for word,misword in zip(bag,miswords):\n",
    "            newsentence= newsentence.replace(word, misword, 1)\n",
    "            #sub_replace(word,misword,newsentence)\n",
    "        if newsentence!=sentence:\n",
    "            ans.append(newsentence)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Это отличное решение для студентов, специалисотв по обработке днных и исследователей в области искусственного интеллекта.',\n",
       " 'Это отличное решение для студентов, специалистов по обработке данных и исследователей в оласти чискусственного интеллекта.']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config={'typos':True,'delchar':True,'addchar':True,'swapchar':True}\n",
    "prepare_words('Это отличное решение для студентов, специалистов по обработке данных и исследователей в области искусственного интеллекта.')\n",
    "augment_sentence('Это отличное решение для студентов, специалистов по обработке данных и исследователей в области искусственного интеллекта.',config,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_words={}\n",
    "def prepare_words(sentences):\n",
    "    reg=re.compile('^[а-я]+$')\n",
    "    for sentence in sentences:\n",
    "        words=re.findall(r\"[\\w']+\", sentence)    \n",
    "        ru_words=[word for word in words if len(word)>3 and reg.match(word)] \n",
    "        for word in ru_words:\n",
    "            if word not in aug_words:\n",
    "                aug_words[word]=make_mistake(word.lower(),config,1)       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_tags(text):\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('((www\\.[^\\s]+)|(http[s]?://[^\\s]+))','url', text)     \n",
    "    text = re.sub('@[^\\s]+','USER', text)\n",
    "    text = re.sub('\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,4}','email', text)   \n",
    "    text = re.sub('(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)','hashtag', text)\n",
    "    text = re.sub('(?:(?:\\d+,?)+(?:\\.?\\d+)?)','num', text)\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return ' '.join(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', text)\n",
    "    text = re.sub('@[^\\s]+','USER', text)\n",
    "    text = text.lower().replace(\"ё\", \"е\")\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return ' '.join(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emo_unicode\n",
    "def convert_emoticons(text):\n",
    "    for emot in emo_unicode.EMOTICONS:\n",
    "        text = re.sub(u'('+emot+')', \"_\".join(emo_unicode.EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_filter_text(data):\n",
    "    data = data.lower()\n",
    "    data = re.sub(r'ё', r'е', data) # ё ----> е\n",
    "    data = re.sub(r'Ё', r'Е', data)\n",
    "    data = re.sub(r'[^а-яА-Я ]',r' ',data) #оставляем только русские буквы и пробелы (все ост символы заменяются на пробел)\n",
    "    data = ' '.join(data.split()) #убираем лишние пробелы    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stops = set(stopwords.words(\"russian\"))\n",
    "snowball = SnowballStemmer(language=\"russian\")\n",
    "\n",
    "def clean_text(data):\n",
    "    delstops = True\n",
    "    simple_filter = True\n",
    "    emoji2word = True\n",
    "    del12gram = True  # True = убираем обрывки слов в 1-2 символа\n",
    "    stem = False\n",
    "    lemma = False    \n",
    "    \n",
    "    if lemma:\n",
    "        data = \" \".join([morph.parse(w)[0].normal_form for w in data.split()])\n",
    "    if stem:\n",
    "        data = \" \".join([snowball.stem(w) for w in data.split()])\n",
    "        \n",
    "    if delstops:\n",
    "        data = \" \".join([w for w in data.split() if w not in stops])\n",
    "    \n",
    "    if simple_filter:\n",
    "        data = preprocess_text(data)\n",
    "    \n",
    "    if del12gram:\n",
    "        data = \" \".join([w for w in data.split() if len(w) > 2])\n",
    "        \n",
    "    if emoji2word:\n",
    "        data = convert_emoticons(data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ru_sent2num(tag):\n",
    "    \"\"\"\n",
    "    document convert to vector (sum of token)\n",
    "    \"\"\"\n",
    "    ans=-1000.0\n",
    "    if 'Ужасно' in tag: \n",
    "        ans=-1.0\n",
    "    elif 'Плохо' in tag: \n",
    "        ans=-0.8\n",
    "    elif 'Хорошо' in tag: \n",
    "        ans=0.8\n",
    "    elif 'Отлично' in tag: \n",
    "        ans=1.0\n",
    "    elif 'Нормально' in tag:\n",
    "        ans=0.0\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ru2_sent2num(tag):\n",
    "    \"\"\"\n",
    "    document convert to vector (sum of token)\n",
    "    \"\"\"\n",
    "    ans=-1000.0\n",
    "    if 'negative' in tag: \n",
    "        ans=-1.0\n",
    "    elif 'positive' in tag: \n",
    "        ans=1.0\n",
    "    elif 'speech' in tag: \n",
    "        ans=0.0\n",
    "    elif 'skip' in tag: \n",
    "        ans=0.0\n",
    "    elif 'neutral' in tag:\n",
    "        ans=0.0\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix2num(tag):\n",
    "    \"\"\"\n",
    "    document convert to vector (sum of token)\n",
    "    \"\"\"\n",
    "    ans=-1000.0\n",
    "    if ('-1' in tag) or ('--' in tag): \n",
    "        ans=-1.0\n",
    "    elif ('1' in tag) or ('++' in tag): \n",
    "        ans=1.0\n",
    "    elif ('0' in tag) or ('-+' in tag) or ('+-' in tag):\n",
    "        ans=0.0\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2num(tag):\n",
    "    \"\"\"\n",
    "    document convert to vector (sum of token)\n",
    "    \"\"\"\n",
    "    ans=-1000.0\n",
    "    if 'negative' in tag: \n",
    "        ans=-1.0\n",
    "    elif 'positive' in tag: \n",
    "        ans=1.0\n",
    "    elif 'neutral' in tag:\n",
    "        ans=0.0\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2num2(tag):\n",
    "    \"\"\"\n",
    "    document convert to vector (sum of token)\n",
    "    \"\"\"\n",
    "    ans=-1000.0\n",
    "    if 'negative' in tag: \n",
    "        ans=-1.0\n",
    "    elif 'positive' in tag: \n",
    "        ans=1.0\n",
    "    elif 'neautral' in tag:\n",
    "        ans=0.0\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_sentence(newlist,size):\n",
    "    res_list=[]\n",
    "    ml=max([len(sent) for sent in newlist])\n",
    "    for i in range(0, ml):\n",
    "        for sentence in newlist:\n",
    "            tmp_list=sentence.split()\n",
    "            l=len(tmp_list)            \n",
    "            if l<i or l<3:\n",
    "                continue\n",
    "            res_list.append(\" \".join(tmp_list[:i]+tmp_list[i+1:]))\n",
    "            if len(res_list)>=size:\n",
    "                return res_list\n",
    "    return res_list          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_word(newlist,size):\n",
    "    res_list=[]\n",
    "    l=len(newlist)\n",
    "    while len(res_list)<size:        \n",
    "        shuff_list=res_list or newlist\n",
    "        for sentence in shuff_list:\n",
    "            rn=np.random.randint(l)\n",
    "            new_word=newlist[rn]\n",
    "            res_list.append(\" \".join([sentence,new_word]))\n",
    "            if len(res_list)>=size:\n",
    "                return res_list\n",
    "    return res_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2num(tag):\n",
    "    \"\"\"\n",
    "    document convert to vector (sum of token)\n",
    "    \"\"\"\n",
    "    ans=0\n",
    "    if tag=='PSTV':\n",
    "        ans=1\n",
    "    elif tag=='NGTV':\n",
    "        ans=-1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_test_2015_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/bank_test.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#0\n",
    "#5296\n",
    "#0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_train_2015_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/bank_train.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "    df['tone'] = df['tone'].apply(mix2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#354\n",
    "#3475\n",
    "#1171\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_train_2016_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/bank_train_2016.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['tone'].apply(mix2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#701\n",
    "#6961\n",
    "#1730\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_test_2016_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/banks_test_2016.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#0\n",
    "#19586\n",
    "#0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bank_test_etalon_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/banks_test_etalon.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#306\n",
    "#2240\n",
    "#767\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tel_train_2015_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/ttk_train.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "    df['tone'] = df['tone'].apply(mix2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "#931\n",
    "#2358\n",
    "#1711\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tel_test_2015_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/ttk_test.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#0\n",
    "#5322\n",
    "#0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tel_test_2016_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/tkk_test_2016.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#0\n",
    "#19673\n",
    "#0\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tel_train_2016_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/tkk_train_2016.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "\n",
    "#1299\n",
    "#4849\n",
    "#2495\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tel_test_etalon_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/tkk_test_etalon.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "#    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "#210\n",
    "#1011\n",
    "#1026\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/news.csv', sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "    df['tone'] = df['sentiment'].apply(sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "#    if balance:\n",
    "#        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,13981)\n",
    "#        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df['tone']=1.0\n",
    "#        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "#        nlist=shuffle_sentence(newlist,16715)\n",
    "#        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "#        tmp_df2['tone']=-1.0\n",
    "#        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "#2795\n",
    "#4034\n",
    "#1434\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rusentiment_dataset(balance=True):\n",
    "    df1 = pd.read_csv('./dataset/rusentiment_random_posts.csv',',')\n",
    "    df2 = pd.read_csv('./dataset/rusentiment_preselected_posts.csv',',') \n",
    "    df3 = pd.read_csv('./dataset/rusentiment_test.csv',',') \n",
    "    df=pd.concat([df1,df2,df3])\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "    df['tone'] = df['label'].apply(ru2_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "    if balance:\n",
    "        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "        nlist=shuffle_sentence(newlist,13981)\n",
    "        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df['tone']=1.0\n",
    "        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "        nlist=shuffle_sentence(newlist,16715)\n",
    "        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df2['tone']=-1.0\n",
    "        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/medical_comments.csv',sep=',')\n",
    "    df['clear_text'] = df['text'].map(clean_text)\n",
    "    df['tone'] = df['sentiment'].apply(ru_sent2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "    if balance:\n",
    "        newlist=df.clear_text[df.tone==0.0].tolist()\n",
    "        nlist=shuffle_sentence(newlist,85806)\n",
    "        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df['tone']=0.0\n",
    "        newlist=df.clear_text[df.tone==-1.0].tolist()\n",
    "        nlist=shuffle_sentence(newlist,28239)\n",
    "        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df2['tone']=-1.0\n",
    "        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clothes_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/women-clothing-accessories.3-class.balanced.txt',sep='\\t')\n",
    "    df['clear_text'] = df['review'].map(clean_text)\n",
    "    df['tone'] = df['sentiment'].apply(sent2num2)\n",
    "    df=df[['clear_text','tone']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emo_dict_old_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/emo_dict_old.csv',sep=';')\n",
    "    df['clear_text'] = df['term'].map(clean_text)\n",
    "    df['tone'] = df['tag'].apply(tag2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "    if balance:\n",
    "        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "        nlist=shuffle_word(newlist,24747)\n",
    "        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df['tone']=1.0\n",
    "        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "        nlist=shuffle_word(newlist,24234)\n",
    "        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df2['tone']=-1.0\n",
    "        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emo_dict_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/emo_dict.csv',sep=';')\n",
    "    df['clear_text'] = df['term'].map(clean_text)\n",
    "    df['tone'] = df['tag'].apply(tag2num)\n",
    "    df=df[['clear_text','tone']]\n",
    "    if balance:\n",
    "        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "        nlist=shuffle_word(newlist,12323)\n",
    "        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df['tone']=1.0\n",
    "        newlist=df.clear_text[df.tone<-0.1].tolist()\n",
    "        nlist=shuffle_word(newlist,10099)\n",
    "        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df2['tone']=-1.0\n",
    "        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_dataset(balance=True):\n",
    "    dtype = {'id': np.uint16, 'date': str, 'name': str, 'text': str,  'type': np.uint8}\n",
    "    ppath = './dataset/positive.csv' #home\n",
    "    npath = './dataset/negative.csv' #home\n",
    "    pdf = pd.read_csv (ppath, sep=';', names=['id', 'date', 'name', 'text', 'type'], usecols=dtype.keys(), dtype=dtype)\n",
    "    ndf = pd.read_csv (npath, sep=';', names=['id', 'date', 'name', 'text', 'type'], usecols=dtype.keys(), dtype=dtype)\n",
    "    pdf.drop(['id', 'date', 'name'], axis=1, inplace=True)\n",
    "    ndf.drop(['id', 'date', 'name'], axis=1, inplace=True)\n",
    "    pdf['clear_text'] = pdf['text'].map(clean_text)\n",
    "    ndf['clear_text'] = ndf['text'].map(clean_text)\n",
    "    pdf.drop('text', axis=1, inplace=True)\n",
    "    ndf.drop('text', axis=1, inplace=True)\n",
    "    ndf['type'] = -1\n",
    "    df = pd.concat([pdf, ndf], ignore_index=True)\n",
    "    df = df.rename(columns={'type': 'tone'})\n",
    "    return df\n",
    "#114911\n",
    "#111923"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def social_dataset(balance=True):\n",
    "    df = pd.read_csv('./dataset/marked.csv',sep=',')\n",
    "    df['clear_text'] = df['FullText'].map(clean_text)\n",
    "    df = df.rename(columns={'Tone': 'tone'})\n",
    "    df=df[['clear_text','tone']]   \n",
    "    if balance:\n",
    "        newlist=df.clear_text[df.tone>0.1].tolist()\n",
    "        nlist=shuffle_sentence(newlist,412)\n",
    "        tmp_df =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df['tone']=1.0\n",
    "        newlist=df.clear_text[df.tone==0.0].tolist()\n",
    "        nlist=shuffle_sentence(newlist,292)\n",
    "        tmp_df2 =pd.DataFrame(nlist,columns=['clear_text'])\n",
    "        tmp_df2['tone']=0.0\n",
    "        df=pd.concat([df,tmp_df,tmp_df2])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rand_ru['vectr'] = rand_ru['clear_text'].apply(vectorize_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_frames(df1,df2):\n",
    "    if df1.shape[0]<1:\n",
    "        return df2\n",
    "    else:\n",
    "        return pd.concat([df1,df2])\n",
    "    \n",
    "def load_dataset(datasets):\n",
    "    df=pd.DataFrame(columns = ['clear_text','tone'])\n",
    "    if datasets['rusentiment'][0]:\n",
    "        df=join_frames(df,rusentiment_dataset(balance=datasets['rusentiment'][1]))\n",
    "    if datasets['medical'][0]:\n",
    "        df=join_frames(df,medical_dataset(balance=datasets['medical'][1]))\n",
    "    if datasets['clothes'][0]:\n",
    "        df=join_frames(df,clothes_dataset(balance=datasets['clothes'][1]))\n",
    "    if datasets['emo_dict_old'][0]:\n",
    "        df=join_frames(df,emo_dict_old_dataset(balance=datasets['emo_dict_old'][1]))\n",
    "    if datasets['emo_dict'][0]:\n",
    "        df=join_frames(df,emo_dict_dataset(balance=datasets['emo_dict'][1]))\n",
    "    if datasets['tweet'][0]:\n",
    "        df=join_frames(df,tweet_dataset(balance=datasets['tweet'][1]))\n",
    "    if datasets['social'][0]:\n",
    "        df=join_frames(df,social_dataset(balance=datasets['social'][1]))\n",
    "    if datasets['bank_test_2015'][0]:\n",
    "        df=join_frames(df,bank_test_2015_dataset(balance=datasets['bank_test_2015'][1]))        \n",
    "    if datasets['bank_train_2015'][0]:\n",
    "        df=join_frames(df,bank_train_2015_dataset(balance=datasets['bank_train_2015'][1]))      \n",
    "    if datasets['bank_train_2016'][0]:\n",
    "        df=join_frames(df,bank_train_2016_dataset(balance=datasets['bank_train_2016'][1]))      \n",
    "    if datasets['bank_test_2016'][0]:\n",
    "        df=join_frames(df,bank_test_2016_dataset(balance=datasets['bank_test_2016'][1]))      \n",
    "    if datasets['bank_test_etalon'][0]:\n",
    "        df=join_frames(df,bank_test_etalon_dataset(balance=datasets['bank_test_etalon'][1]))              \n",
    "    if datasets['tel_train_2015'][0]:\n",
    "        df=join_frames(df,tel_train_2015_dataset(balance=datasets['tel_train_2015'][1]))   \n",
    "    if datasets['tel_test_2015'][0]:\n",
    "        df=join_frames(df,tel_test_2015_dataset(balance=datasets['tel_test_2015'][1]))                   \n",
    "    if datasets['tel_test_2016'][0]:\n",
    "        df=join_frames(df,tel_test_2016_dataset(balance=datasets['tel_test_2016'][1]))                   \n",
    "    if datasets['tel_train_2016'][0]:\n",
    "        df=join_frames(df,tel_train_2016_dataset(balance=datasets['tel_train_2016'][1]))                   \n",
    "    if datasets['tel_test_etalon'][0]:\n",
    "        df=join_frames(df,tel_test_etalon_dataset(balance=datasets['tel_test_etalon'][1]))           \n",
    "    if datasets['news'][0]:\n",
    "        df=join_frames(df,news_dataset(balance=datasets['news'][1]))                   \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_model(df_train,df_test):\n",
    "    df_train['vectr'] = df_train['clear_text'].apply(vectorize_sum)\n",
    "    df_test['vectr'] = df_test['clear_text'].apply(vectorize_sum)\n",
    "    X=df_train['vectr'].tolist()\n",
    "    Y=df_train['tone'].tolist()\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    x_hat=df_test['vectr'].tolist()\n",
    "    y_hat=model.predict(x_hat)\n",
    "    return y_hat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(df_train,df_test):   #emb. TF-IDF 0.7303\n",
    "    y_train = np.asarray(df_train['tone'],dtype=np.int8)\n",
    "    train_data = np.array([twitt for twitt in df_train['clear_text']])\n",
    "    test_data = np.array([twitt for twitt in df_test['clear_text']])\n",
    "    \n",
    "    tfidf = TfidfVectorizer(\n",
    "           ngram_range=(1, 3),\n",
    "           use_idf=1,\n",
    "           smooth_idf=1)\n",
    "\n",
    "    data_train_count = tfidf.fit_transform(train_data)\n",
    "    data_test_count  = tfidf.transform(test_data)\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(data_train_count, y_train)\n",
    "    pred = clf.predict(data_test_count)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes2(df_train,df_test):\n",
    "    y_train = np.asarray(df_train['tone'],dtype=np.int8)\n",
    "    train_data = np.array([twitt for twitt in df_train['clear_text']])\n",
    "    test_data = np.array([twitt for twitt in df_test['clear_text']])\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),])\n",
    "    text_clf.fit(train_data, y_train)\n",
    "    predicted = text_clf.predict(test_data)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes_gs(df_train,df_test):\n",
    "    y_train = np.asarray(df_train['tone'],dtype=np.int8)\n",
    "    train_data = np.array([twitt for twitt in df_train['clear_text']])\n",
    "    test_data = np.array([twitt for twitt in df_test['clear_text']])\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)),])\n",
    "    parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "                  'tfidf__use_idf': (True, False),\n",
    "                  'clf__alpha': (1e-2, 1e-3),}\n",
    "    gs_clf = GridSearchCV(text_clf, parameters, cv=5, n_jobs=16,verbose=1)\n",
    "    gs_clf.fit(train_data, y_train)\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "    predicted = gs_clf.predict(test_data)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(df_train,df_test):\n",
    "    y_train = np.asarray(df_train['tone'],dtype=np.int8)\n",
    "    train_data = np.array([twitt for twitt in df_train['clear_text']])\n",
    "    test_data = np.array([twitt for twitt in df_test['clear_text']])\n",
    "    \n",
    "    tfidf = TfidfVectorizer(\n",
    "           ngram_range=(1, 3),\n",
    "           use_idf=1,\n",
    "           smooth_idf=1)\n",
    "\n",
    "    data_train_count = tfidf.fit_transform(train_data)\n",
    "    data_test_count  = tfidf.transform(test_data)\n",
    "    \n",
    "    classifier_linear = LinearSVC()\n",
    "    classifier_linear.fit(data_train_count, y_train)\n",
    "    pred = classifier_linear.predict(data_test_count)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_class_count(df_train,df_test):\n",
    "    ngd=defaultdict(float)\n",
    "    psd=defaultdict(float)\n",
    "    ntd=defaultdict(float)    \n",
    "    y_train = df_train['tone']\n",
    "    train_data = df_train['clear_text'].tolist()\n",
    "    for x,y in zip(train_data, y_train):        \n",
    "        if y>0.1:\n",
    "            for w in x.split():\n",
    "                psd[w] += 1.0\n",
    "        elif y<-0.1:\n",
    "            for w in x.split():\n",
    "                ngd[w] += 1.0  \n",
    "        else:\n",
    "            for w in x.split():\n",
    "                ntd[w] += 1.0\n",
    "    words=set(list(ngd.keys())+list(psd.keys())+list(ntd.keys()))\n",
    "    words=list(words)\n",
    "    wl=sum(ngd.values())+sum(psd.values())+sum(ntd.values())\n",
    "    for w in words:\n",
    "        s=psd[w] + ntd[w] + ngd[w]+1\n",
    "        psd[w] = (psd[w]+1)/s\n",
    "        ngd[w] = (ngd[w]+1)/s\n",
    "        ntd[w] = (ntd[w]+1)/s\n",
    "    \n",
    "    res=[]\n",
    "    for sent in df_test['clear_text']:   \n",
    "        pos=ngt=neut=0\n",
    "        for w in sent.split():\n",
    "            pos+=psd[w]\n",
    "            ngt+=ngd[w]\n",
    "            neut+=ntd[w]\n",
    "        if pos>=ngt and pos>=neut:\n",
    "            res.append(1)\n",
    "        elif ngt>=pos and ngt>=neut:\n",
    "            res.append(-1)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset={'rusentiment':(True,False),\\\n",
    "               'medical':(False,False),\\\n",
    "               'clothes':(False,False),\\\n",
    "               'emo_dict_old':(False,False),\\\n",
    "               'emo_dict':(False,False),\\\n",
    "               'tweet':(False,False),\\\n",
    "               'social':(False,False),\\\n",
    "               'news':(False,False),\\\n",
    "               'bank_test_2015':(False,False),\\\n",
    "               'bank_train_2015':(False,False),\\\n",
    "               'bank_train_2016':(False,False),\\\n",
    "               'bank_test_2016':(False,False),\\\n",
    "               'bank_test_etalon':(False,False),\\\n",
    "               'tel_train_2015':(True,False),\\\n",
    "               'tel_test_2015':(True,False),\\\n",
    "               'tel_test_2016':(True,False),\\\n",
    "               'tel_train_2016':(True,False),\\\n",
    "               'tel_test_etalon':(True,False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset(whole_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9086\n",
      "53840\n",
      "9144\n"
     ]
    }
   ],
   "source": [
    "print(df.clear_text[df.tone  > 0.1].count())\n",
    "print(df.clear_text[df.tone  == 0.0].count())\n",
    "print(df.clear_text[df.tone  < -0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = pd.read_csv('./dataset/spam.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24384"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam=df_spam[['clear_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam['clear_text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam.dropna(subset=['clear_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24384"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam=df_spam.rename(columns={\"clear_text\": \"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm = pd.read_csv('./dataset/norm.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm['text'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm.dropna(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31185"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_norm=df_norm[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "posetive=df_norm.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative=df_spam.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_words(posetive+negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_text=[]\n",
    "config={'typos':True,'delchar':True,'addchar':True,'swapchar':True}\n",
    "for sentence in posetive:\n",
    "    new_sentence=augment_sentence(sentence,config,2,3)\n",
    "    aug_text+=new_sentence\n",
    "aug_text=aug_text+posetive\n",
    "df_norm=pd.DataFrame(aug_text) \n",
    "df_norm[\"spam\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65160"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_text=[]\n",
    "config={'typos':True,'delchar':True,'addchar':True,'swapchar':True}\n",
    "for sentence in negative:\n",
    "    new_sentence=augment_sentence(sentence,config,3,3)\n",
    "    aug_text+=new_sentence\n",
    "aug_text=aug_text+negative\n",
    "df_spam=pd.DataFrame(aug_text) \n",
    "df_spam[\"spam\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65160\n",
      "59053\n"
     ]
    }
   ],
   "source": [
    "print(len(df_norm))\n",
    "print(len(df_spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_norm,df_spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124213"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>А попа подозревала двано,что ты с кавказса..пе...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>А попа подозревала давно,что ты с кавказа..пер...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Блин, почему эта жизнь стоь не справедлива ((((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Блин, почему эта жизнь столь не спарведлива ((((</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>где еще встречатйь свой день рождения как не н...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59048</th>\n",
       "      <td>sms предупрежден мангистауск дчс направл абоне...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59049</th>\n",
       "      <td>sms предупрежден мангистауск дчс направл абоне...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59050</th>\n",
       "      <td>номер восстанавлива</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59051</th>\n",
       "      <td>beautiful прекрасн</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59052</th>\n",
       "      <td>bespantovy беспантов</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>124213 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0 spam\n",
       "0      А попа подозревала двано,что ты с кавказса..пе...    0\n",
       "1      А попа подозревала давно,что ты с кавказа..пер...    0\n",
       "2        Блин, почему эта жизнь стоь не справедлива ((((    0\n",
       "3       Блин, почему эта жизнь столь не спарведлива ((((    0\n",
       "4      где еще встречатйь свой день рождения как не н...    0\n",
       "...                                                  ...  ...\n",
       "59048  sms предупрежден мангистауск дчс направл абоне...    1\n",
       "59049  sms предупрежден мангистауск дчс направл абоне...    1\n",
       "59050                                номер восстанавлива    1\n",
       "59051                                 beautiful прекрасн    1\n",
       "59052                               bespantovy беспантов    1\n",
       "\n",
       "[124213 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[0].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124213"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(num_instances)<0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test=df[msk],df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111759"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12454"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "df_test.spam = df_test.spam.astype(int)\n",
    "df_train.spam = df_train.spam.astype(int)\n",
    "\n",
    "df_train.to_csv('spam_train.csv', index=False, header=None)\n",
    "df_test.to_csv('spam_test.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances=len(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(num_instances)<0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test=df_spam[msk],df_spam[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130712"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14510"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65790\n",
      "64850\n"
     ]
    }
   ],
   "source": [
    "print(df_train.clear_text[df_train.spam  == 1].count())\n",
    "print(df_train.clear_text[df_train.spam  == 0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7323\n",
      "7176\n"
     ]
    }
   ],
   "source": [
    "print(df_test.clear_text[df_test.spam  == 1].count())\n",
    "print(df_test.clear_text[df_test.spam  == 0].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'clear_text': str,'tone': np.int8} \n",
    "df = pd.read_csv('all_test.tsv', sep='\\t', dtype=dtype, names=['clear_text', 'tone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clear_text</th>\n",
       "      <th>tone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>семизубая девчонка это</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>днем варенья фанат всего самого наилутшего</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>рождение это великое чудо</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>вот тебе няшку</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>едут автобусе панк монашка панк говорит типа т...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18501</th>\n",
       "      <td>мтс связываюсь доп соглашений amp raquo дикими...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18502</th>\n",
       "      <td>user ума чтоль сошли мне августа связи url</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18503</th>\n",
       "      <td>user билайн это оператор состояние души предло...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18504</th>\n",
       "      <td>user здравствуйте мобильным интернетом караган...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18505</th>\n",
       "      <td>user спасибо просто ростелекоме это стоит рубл...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clear_text  tone\n",
       "0                                 семизубая девчонка это   1.0\n",
       "1             днем варенья фанат всего самого наилутшего   1.0\n",
       "2                              рождение это великое чудо   2.0\n",
       "3                                         вот тебе няшку   2.0\n",
       "4      едут автобусе панк монашка панк говорит типа т...   1.0\n",
       "...                                                  ...   ...\n",
       "18501  мтс связываюсь доп соглашений amp raquo дикими...   0.0\n",
       "18502         user ума чтоль сошли мне августа связи url   0.0\n",
       "18503  user билайн это оператор состояние души предло...   0.0\n",
       "18504  user здравствуйте мобильным интернетом караган...   0.0\n",
       "18505  user спасибо просто ростелекоме это стоит рубл...   0.0\n",
       "\n",
       "[18506 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tone'] = df.tone + 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('all_test_new.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('spam_train.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)\n",
    "df_test.to_csv('spam_test.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clear_text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>попа подозревала давно что кавказа перестану о...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>прошедшим днем ангела</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>два дня отлета острова</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>блин почему эта жизнь столь справедлива</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>встречать свой день рождения кладбище</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>мтс заботится моем бюджете просто позволяя вто...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2243</th>\n",
       "      <td>караоке ростелекома</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>user включили подключении дублирующую опцию ка...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2245</th>\n",
       "      <td>агенты приватизации шереметьево ростелекома по...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2246</th>\n",
       "      <td>вообще странно конечно несколько омске последн...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72070 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             clear_text  spam\n",
       "0     попа подозревала давно что кавказа перестану о...     0\n",
       "1                                 прошедшим днем ангела     0\n",
       "2                                два дня отлета острова     0\n",
       "3               блин почему эта жизнь столь справедлива     0\n",
       "4                 встречать свой день рождения кладбище     0\n",
       "...                                                 ...   ...\n",
       "2242  мтс заботится моем бюджете просто позволяя вто...     0\n",
       "2243                                караоке ростелекома     0\n",
       "2244  user включили подключении дублирующую опцию ка...     0\n",
       "2245  агенты приватизации шереметьево ростелекома по...     0\n",
       "2246  вообще странно конечно несколько омске последн...     0\n",
       "\n",
       "[72070 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=pd.concat([df,df_spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clear_text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>попа подозревала давно что кавказа перестану о...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>прошедшим днем ангела</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>два дня отлета острова</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>блин почему эта жизнь столь справедлива</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>встречать свой день рождения кладбище</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145217</th>\n",
       "      <td>sms предупрежден мангистауск дчс направл абоне...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145218</th>\n",
       "      <td>sms предупрежден мангистауск дчс направл абоне...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145219</th>\n",
       "      <td>номер восстанавлива</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145220</th>\n",
       "      <td>beautiful прекрасн</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145221</th>\n",
       "      <td>bespantovy беспантов</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145222 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clear_text  spam\n",
       "0       попа подозревала давно что кавказа перестану о...     0\n",
       "1                                   прошедшим днем ангела     0\n",
       "2                                  два дня отлета острова     0\n",
       "3                 блин почему эта жизнь столь справедлива     0\n",
       "4                   встречать свой день рождения кладбище     0\n",
       "...                                                   ...   ...\n",
       "145217  sms предупрежден мангистауск дчс направл абоне...     1\n",
       "145218  sms предупрежден мангистауск дчс направл абоне...     1\n",
       "145219                                номер восстанавлива     1\n",
       "145220                                 beautiful прекрасн     1\n",
       "145221                               bespantovy беспантов     1\n",
       "\n",
       "[145222 rows x 2 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.to_csv('all_test.tsv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=pdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf=pdf.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam = pd.read_csv('./dataset/spam_dataset.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam['clear_text'] = df_spam['clear_text'].map(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam=df_spam.clear_text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24384"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_text=[]\n",
    "config={'typos':True,'delchar':True,'addchar':True,'swapchar':True}\n",
    "for sentence in spam:\n",
    "    new_sentence=augment_sentence(sentence,config,2,3)\n",
    "    aug_text+=new_sentence\n",
    "aug_text=aug_text+spam\n",
    "df_spam=pd.DataFrame(aug_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam[\"spam\"]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spam=df_spam.rename(columns={0: \"clear_text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clear_text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>распоряжеини cnews оказалась обновленная аерси...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>распоряжении cnews оказалась обновленная верси...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>скидка оплату премичум дотсупа играм билайн пр...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>сёкидка оплату премиум доступа играм блайн про...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>всем привет хочешь мной поиграть пиши вконтакт...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73147</th>\n",
       "      <td>sms предупрежден мангистауск дчс направл абоне...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73148</th>\n",
       "      <td>sms предупрежден мангистауск дчс направл абоне...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73149</th>\n",
       "      <td>номер восстанавлива</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73150</th>\n",
       "      <td>beautiful прекрасн</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73151</th>\n",
       "      <td>bespantovy беспантов</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73152 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clear_text  spam\n",
       "0      распоряжеини cnews оказалась обновленная аерси...     1\n",
       "1      распоряжении cnews оказалась обновленная верси...     1\n",
       "2      скидка оплату премичум дотсупа играм билайн пр...     1\n",
       "3      сёкидка оплату премиум доступа играм блайн про...     1\n",
       "4      всем привет хочешь мной поиграть пиши вконтакт...     1\n",
       "...                                                  ...   ...\n",
       "73147  sms предупрежден мангистауск дчс направл абоне...     1\n",
       "73148  sms предупрежден мангистауск дчс направл абоне...     1\n",
       "73149                                номер восстанавлива     1\n",
       "73150                                 beautiful прекрасн     1\n",
       "73151                               bespantovy беспантов     1\n",
       "\n",
       "[73152 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72070"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131841\n",
      "108708\n",
      "129747\n"
     ]
    }
   ],
   "source": [
    "print(df.clear_text[df.tone  > 0.1].count())\n",
    "print(df.clear_text[df.tone  == 0.0].count())\n",
    "print(df.clear_text[df.tone  < -0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instances=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#msk = [False]*num_instances\n",
    "msk = np.random.rand(num_instances)<0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test=df[msk],df[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125132\n",
      "103266\n",
      "123274\n"
     ]
    }
   ],
   "source": [
    "print(df_train.clear_text[df_train.tone  > 0.1].count())\n",
    "print(df_train.clear_text[df_train.tone  == 0.0].count())\n",
    "print(df_train.clear_text[df_train.tone  < -0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6709\n",
      "5442\n",
      "6473\n"
     ]
    }
   ],
   "source": [
    "print(df_test.clear_text[df_test.tone  > 0.1].count())\n",
    "print(df_test.clear_text[df_test.tone  == 0.0].count())\n",
    "print(df_test.clear_text[df_test.tone  < -0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug=df_train.copy()\n",
    "df_aug['clear_text'] = df_aug['clear_text'].apply(lambda x: str(augment_sentence(x,config,1,10)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_aug.clear_text[df_aug.tone  > 0.1].count())\n",
    "print(df_aug.clear_text[df_aug.tone  == 0.0].count())\n",
    "print(df_aug.clear_text[df_aug.tone  < -0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df_train,df_aug])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "df_test['tone'] = df_test.tone + 1.0\n",
    "df_test.tone = df_test.tone.astype(int)\n",
    "df.tone = df.tone.astype(int)\n",
    "\n",
    "df.to_csv('all_train.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)\n",
    "df_test.to_csv('all_test.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "posetive=df_train.clear_text[df_train.tone  > 0.1].tolist()\n",
    "neutral=df_train.clear_text[df_train.tone  == 0.0].tolist()\n",
    "negative=df_train.clear_text[df_train.tone  < -0.1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posetive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_text=[]\n",
    "config={'typos':True,'delchar':True,'addchar':True,'swapchar':True}\n",
    "reg=re.compile('^[а-я]+$')\n",
    "for sentence in posetive:\n",
    "    words=re.findall(r\"[\\w']+\", sentence)\n",
    "    words=[word for word in words if len(word)>3 and reg.match(word) ]  \n",
    "    size=len(words)\n",
    "    new_examples=size//7+1\n",
    "    new_sentence=augment_sentence(sentence,config,new_examples,3)\n",
    "    aug_text+=new_sentence\n",
    "aug_text=aug_text+posetive\n",
    "df_posetive=pd.DataFrame(aug_text) \n",
    "df_posetive[\"tone\"]='2'\n",
    "\n",
    "aug_text=[]\n",
    "for sentence in neutral:\n",
    "    words=re.findall(r\"[\\w']+\", sentence)\n",
    "    words=[word for word in words if len(word)>3 and reg.match(word) ]  \n",
    "    size=len(words)\n",
    "    new_examples=size//7+1\n",
    "    new_sentence=augment_sentence(sentence,config,new_examples,3)\n",
    "    aug_text+=new_sentence\n",
    "aug_text=aug_text+neutral\n",
    "df_neutral=pd.DataFrame(aug_text) \n",
    "df_neutral[\"tone\"]='1'\n",
    "\n",
    "aug_text=[]\n",
    "for sentence in negative:\n",
    "    words=re.findall(r\"[\\w']+\", sentence)\n",
    "    words=[word for word in words if len(word)>3 and reg.match(word) ]  \n",
    "    size=len(words)\n",
    "    new_examples=size//7+1\n",
    "    new_sentence=augment_sentence(sentence,config,new_examples,3)\n",
    "    aug_text+=new_sentence\n",
    "aug_text=aug_text+negative\n",
    "df_negative=pd.DataFrame(aug_text) \n",
    "df_negative[\"tone\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_posetive))\n",
    "print(len(df_neutral))\n",
    "print(len(df_negative))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.concat([df_posetive,df_neutral,df_negative])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.clear_text[df_train.tone  > 0.1].count())\n",
    "print(df_train.clear_text[df_train.tone  == 0.0].count())\n",
    "print(df_train.clear_text[df_train.tone  < -0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "df_test['tone'] = df_test.tone + 1.0\n",
    "df_test.tone = df_test.tone.astype(int)\n",
    "df_train.tone = df_train.tone.astype(int)\n",
    "\n",
    "df_train.to_csv('all_train.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)\n",
    "df_test.to_csv('all_test.tsv', quoting=csv.QUOTE_NONE,index=False, sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devenv",
   "language": "python",
   "name": "devenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
